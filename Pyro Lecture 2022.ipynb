{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935f4d56",
   "metadata": {},
   "source": [
    "# A Quick Refresher on Probabilistic Machine Learning\n",
    "\n",
    "Most data analysis problems can be understood as elaborations on three basic high-level questions:\n",
    "\n",
    "    What do we know about the problem before observing any data?\n",
    "\n",
    "    What conclusions can we draw from data given our prior knowledge?\n",
    "\n",
    "    Do these conclusions make sense?\n",
    "\n",
    "In the probabilistic or Bayesian approach to data science and machine learning, we formalize these in terms of mathematical operations on probability distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6dc58f",
   "metadata": {},
   "source": [
    "### A probabilistic model\n",
    "First, we express everything we know about the variables in a problem and the relationships between them in the form of a probabilistic model, or a joint probability distribution over a collection of random variables. A model has observations $\\mathbf{x}$ and latent random variables $\\mathbf{z}$ as well as parameters $\\theta$. It usually has a joint density function of the form: \n",
    "$$ \n",
    "    p_{\\theta} (\\mathbf{x}, \\mathbf{z}) = p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z}) p_{\\theta} (\\mathbf{z})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9395d39",
   "metadata": {},
   "source": [
    "Probabilistic models are often depicted in a standard graphical notation (i.e., as a Probabilistic Graphical Model (PGM)) for visualization and communication, summarized below, although it is possible to conceive models that do not have a fixed graphical structure.\n",
    "\n",
    "Below is an example of a PGM for the Latent Dirichlet Allocation model [1]:\n",
    "![Latent Dirichlet Allocation](https://upload.wikimedia.org/wikipedia/commons/d/d3/Latent_Dirichlet_allocation.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909e86b",
   "metadata": {},
   "source": [
    "### Performing inference on the model\n",
    "Once we have specified a model, Bayes’ rule tells us how to use it to perform inference, or draw conclusions about latent variables from data, by computing the posterior distribution over $\\mathbf{z}$:\n",
    "$$\n",
    "    p_{\\theta} (\\mathbf{z} \\mid \\mathbf{x}) = \\frac{p_{\\theta} (\\mathbf{x}, \\mathbf{z})}{\\int p_{\\theta} (\\mathbf{x}, \\mathbf{z}) d\\mathbf{z}}\n",
    "$$\n",
    "The denominator is called model evidence:\n",
    "$$\n",
    "\\int p_{\\theta} (\\mathbf{x}, \\mathbf{z}) d\\mathbf{z} = p_{\\theta} (\\mathbf{x})\n",
    "$$\n",
    "We would also like to learn the parameters $\\theta$ that best fit the data, which we can do by maximizing log evidence:\n",
    "\n",
    "$$\n",
    "    \\theta_{max} = \\underset{\\theta}{\\operatorname{argmax}} \\log p_{\\theta} (\\mathbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503f714",
   "metadata": {},
   "source": [
    "### The issue of computing evidence\n",
    "The evidence is often difficult to compute (analytically untractable), but we need to do it to be able to perform inference. To solve this hurdle we can resort to Approximate Bayesian Inference (ABI) where distributions difficult to compute are approximated. Roughly, it is comprised of two main groups of techniques: \n",
    "  - sampling based methods (Markov Chain Monte Carlo (MCMC))\n",
    "  - optimization based methods (Variational Inference (VI))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a47f9",
   "metadata": {},
   "source": [
    "# Variational Inference (VI)\n",
    "\n",
    "![variational_inference](img/variational_inference.png)\n",
    "\n",
    "### When to use VI, and when to use MCMC?\n",
    "Let us refer to David Blei's advice on the matter, as presented in [2]:\n",
    "\n",
    "**\"Compared to MCMC, variational inference tends to be faster and easier to scale to large data. But variational inference has been studied less rigorously than MCMC, and its statistical properties are less well understood. MCMC methods tend to be more computationally intensive than variational inference but they also provide guarantees of producing (asymptotically) exact samples from the target density. Variational inference does not enjoy such guarantees, it can only find a density close to the target, but tends to be faster than MCMC. Because it rests on optimization, variational inference easily takes advantage of methods like stochastic optimization.\"**\n",
    "\n",
    "**\"For example, we might use MCMC in a setting where we spent 20 years collecting a small but expensive data set, where we are confident that our model is appropriate, and where we require precise inferences. We might use variational inference when fitting a probabilistic model of text to one billion text documents and where the inferences will be used to serve search results to a large population of users.\"**\n",
    "\n",
    "\n",
    "### Fundamental idea behind VI\n",
    "The underlying paradigm is that of using *optimization* instead of *sampling* to arrive at an approximate posterior. The idea behind variational inference is to select a family of distributions $\\mathcal{Q}$ parameterized by a parameter set $\\lambda$, from which we choose a member $q_{\\lambda^*}(\\mathbf{z})$ that minimizes a distance measure (the Kullback-Leibler divergence) between the approximate posterior and the exact posterior:\n",
    "$$\n",
    "    q_{\\lambda^*}(\\mathbf{z}) = \\underset{q_{\\lambda}(\\mathbf{z}) \\in \\mathcal{Q}}{\\operatorname{argmin}} \\textrm{KL} (q_{\\lambda}(\\mathbf{z}) \\mid p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x}))\n",
    "$$\n",
    "\n",
    "We can call $\\mathcal{Q}$ the *variational family*, $q$ the *variational distribution*, and $\\lambda$ the *variational parameters*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5617f93d",
   "metadata": {},
   "source": [
    "### ELBO\n",
    "This objective cannot be optimized directly, as it depends on the log evidence. We can instead mximize a proxy objective called the Evidence Lower Bound (ELBO):\n",
    "$$\n",
    "    \\textrm{ELBO} = \\mathbb{E}_{q_{\\lambda} (\\mathbf{z})} \\left[ \\log p_{\\theta} (\\mathbf{x}, \\mathbf{z}) - \\log q_{\\lambda} (\\mathbf{z}) \\right]\n",
    "$$\n",
    "\n",
    "The connection between the two is:\n",
    "$$\n",
    "    \\log p_{\\theta} (\\mathbf{x}) - \\textrm{ELBO} = \\textrm{KL} (q_{\\lambda}(\\mathbf{z}) \\mid p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x}))\n",
    "$$\n",
    "\n",
    "Our optimization objective is now to maximize $\\textrm{ELBO}$ or minimize $- \\textrm{ELBO}$ by moving in the $\\theta$ - $\\lambda$ parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5862710",
   "metadata": {},
   "source": [
    "### Stochastic Optimization\n",
    "\n",
    "![SGD](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/gradient-descent-range.png)\n",
    "\n",
    "Hence, to proceed we must choose an optimization method for our objective function. In modern machine learning **stochastic gradient descent (SGD)** is widely used as an optimization method.\n",
    "Let $f(\\dots)$ be an objective function to minimize, $h_t (x)$ the realization of a RV $H(x)$ whose expectation is the gradient of $f$, and $\\rho_t$ a non-negative scalar. Then we can define stochastic optimization as iterative updates of parameters of the form [3]:\n",
    "$$\n",
    "    x_{t+1} \\leftarrow x_t - \\rho_t h_t(x)\n",
    "$$\n",
    "Under certain conditions on $\\rho_t$, $f(x)$ converges to a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754a0c9",
   "metadata": {},
   "source": [
    "### Wouldn't it be nice...\n",
    "\n",
    " - to have a programmatic way to express **ANY** probabilistic program\n",
    " - have at our disposal a series of inference algorithms that are model agnostic\n",
    " \n",
    "Probabilistic programming languages (PPL) do exactly that! They offer a universal way to express probabilistic programs, and furthermore offer general purpose inference algorithms that are abstracted away from the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e70e24",
   "metadata": {},
   "source": [
    "# The building blocks of a PPL\n",
    "\n",
    "## Stochastic primitives\n",
    "Stochastic primitives, also called Elementary Random Procedures (ERP) are a set of functions/services that a PPL offers that fulfill fundamental operations:\n",
    " + sampling,\n",
    " + conditioning on data (observations),\n",
    " + declaring a variable a parameter admitting optimization (the parameter is learnable).\n",
    "\n",
    "## Distributions\n",
    "We assume that the various probability distributions $d_i$ we use to define our model $p_{\\theta} (\\mathbf{x}, \\mathbf{z})$ have the following properties (speaking from a computational/implementation point of view):\n",
    " - we can sample from each $d_i$,\n",
    " - we can compute the pointwise log pdf $\\log d_i$,\n",
    " - is differentiable w.r.t. the parameters $\\theta$.\n",
    "\n",
    "## Defining models\n",
    "A model is fundamentally an implementation of a stochastic function taking some arguments. We can thus treat it as any other function, but its outputs are going to be stochastic, i.e. they'll likely change from one function call to another. Calling this function effectively corresponds to sampling from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9348d",
   "metadata": {},
   "source": [
    "## The public interface of a PPL\n",
    "Let an imaginary PPL library be called **amazingppl**.\n",
    "\n",
    "Let sampling be done through a call to:\n",
    "```python\n",
    "def sample(name, fn, *args, **kwargs):\n",
    "    ...\n",
    "```\n",
    "And let conditioning be done through an optional argument to sample:\n",
    "```python\n",
    "amazingppl.sample(\"rv_1\", fn=..., obs=some_data)\n",
    "```\n",
    "The argument fn is a distribution object satisfying the above implementation requirements.\n",
    "\n",
    "Sampling without conditioning corresponds to declaring a RV latent (i.e. $\\mathbf{z}$), while sampling with conditioning declares it observed (i.e. $\\mathbf{x}$). The two together allow us to create generic probabilistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2cd74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model of Bayesian linear regression\n",
    "def bayesian_lr_model(is_cont_africa, ruggedness, log_gdp):\n",
    "    # Sample intercept\n",
    "    a = amazingppl.sample(\"a\", dist.Normal(0., 10.))\n",
    "    \n",
    "    # Sample regression coefficients\n",
    "    b_a = amazingppl.sample(\"bA\", dist.Normal(0., 1.))\n",
    "    b_r = amazingppl.sample(\"bR\", dist.Normal(0., 1.))\n",
    "    b_ar = amazingppl.sample(\"bAR\", dist.Normal(0., 1.))\n",
    "    \n",
    "    # Mean and std dev of the response variable\n",
    "    sigma = amazingppl.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "    mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness\n",
    "    \n",
    "    # Declare data conditionally independent\n",
    "    with amazingppl.plate(\"data\", len(ruggedness), dim=-1):\n",
    "        amazingppl.sample(\"obs\", dist.Normal(mean, sigma), obs=log_gdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bfb5df",
   "metadata": {},
   "source": [
    "Here we've added another library function - *plates*. They are syntactic sugar for declaring a dimension conditionally independent, and could basically be replaced by a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281e080",
   "metadata": {},
   "source": [
    "# Defining a variational family, variational distribution, and variational parameters\n",
    "\n",
    "The definition of the variational distribution is going to be similar as that of the model. It is a stochastic function with **sample** statements, but with two distinctions:\n",
    " 1. Recall that the variational distribution $q (\\mathbf{z})$ is defined only over the latent variables $\\mathbf{z}$. Hence, we can't have observed sampled variables.\n",
    " 2. We declare learnable parameters that parameterise the distributions of sample statements over $\\mathbf{z}$. We can implement this with the **param** primitive function:\n",
    "    ```python\n",
    "        def param(\n",
    "            name, init_value, constraint,\n",
    "        )\n",
    "     ```\n",
    "     Parameters can be explicitly constrained to various subsets of $\\mathbb{R}^n$ by using torch.distributions.constraints.Constraint.\n",
    " 3. Sample statements in the variational distribution implementation need to correspond by name to the latent variables in the defined model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b1361a",
   "metadata": {},
   "source": [
    "For our example let us choose a mean field variational distribution. That is, we assume that the variational distribution factorizes as:\n",
    "$$\n",
    "    q(z_1, \\dots, z_m) = \\prod_{i=1}^m q(z_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b84094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simple guide for the Bayesian linear regression model\n",
    "def bayesian_lr_var_dist(is_cont_africa, ruggedness, log_gdp):\n",
    "    # Intercept distribution parameters\n",
    "    a_loc = amazingppl.param(\n",
    "        'a_loc', torch.tensor(0.)\n",
    "    )\n",
    "    a_scale = amazingppl.param(\n",
    "        'a_scale', torch.tensor(1.), constraint=constraints.positive\n",
    "    )\n",
    "    \n",
    "    # Std dev parameter\n",
    "    sigma_loc = amazingppl.param(\n",
    "        'sigma_loc', torch.tensor(1.), constraint=constraints.positive\n",
    "    )\n",
    "    \n",
    "    # Regression coefficients distribution parameters\n",
    "    weights_loc = amazingppl.param(\n",
    "        'weights_loc', torch.randn(3)\n",
    "    )\n",
    "    weights_scale = amazingppl.param(\n",
    "        'weights_scale', torch.ones(3), constraint=constraints.positive\n",
    "    )\n",
    "    \n",
    "    # Sample from the variational distribution using the variational parameters\n",
    "    a = amazingppl.sample(\"a\", dist.Normal(a_loc, a_scale))\n",
    "    \n",
    "    b_a = amazingppl.sample(\"bA\", dist.Normal(weights_loc[0], weights_scale[0]))\n",
    "    b_r = amazingppl.sample(\"bR\", dist.Normal(weights_loc[1], weights_scale[1]))\n",
    "    b_ar = amazingppl.sample(\"bAR\", dist.Normal(weights_loc[2], weights_scale[2]))\n",
    "    \n",
    "    sigma = amazingppl.sample(\"sigma\", dist.Normal(sigma_loc, torch.tensor(0.05)))\n",
    "    # mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e8342",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Aside from the definition of the model and the variational distribution as stochastic functions using stochastic primitives of our library, the remaining key abstractions for inference are:\n",
    " - **the optimizer** - responsible for keeping track of parameter values and updating their values based on the computed gradients\n",
    " - **the objective function** - the ELBO\n",
    " - **the object abstracting inference** - \"SVI\" class taking the model, the VD, optimizer and objective function, and handling all computation given some data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d8f2d",
   "metadata": {},
   "source": [
    "# Probabilistic Programming in Pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1231c731",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtorch_functional\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as torch_functional\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.contrib.examples.util import MNIST\n",
    "from pyro.infer import SVI, Trace_ELBO \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f225d",
   "metadata": {},
   "source": [
    "[Pyro](https://pyro.ai/) is a universal probabilistic programming language built in Python that uses PyTorch as its backend. Among its advantages are that it's:\n",
    " + **Universal**: it can represent any computable probability distribution - *\"A “universal PPL” is an extension of a Turing-complete general-purpose language, which can express models with an unbounded number of random variables. This means that random variables are not fixed statically in the model (as they are in a finite PGM) but can be created dynamically during execution.\"*\n",
    " + **Scalable**: scales to large data sets with little overhead\n",
    " + **Minimal**: implemented with a small core of powerful, composable abstractions\n",
    " \n",
    "A feature of Pyro making it particularly appealing is the ease in developing deep probabilistic models. Seamless integration with PyTorch makes this task trivial in Pyro, thus bridging the world of Bayesian modelling and deep learning. To illustrate this we'll develop an implementation of the Variational Autoencoder (VAE) first in PyTorch, and follow that up with an implementation in Pyro. This will allow us to objectively assess the advantages/disadvantages of one or the other approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce7292",
   "metadata": {},
   "source": [
    "# VAE: A Refresher on Theory\n",
    "The variational autoencoder (VAE) is arguably the simplest setup that realizes deep probabilistic modeling. Below can be seen a PGM of the simplest form of the model. Having observed $N$ data points denoted $x_i$, we pose a model where each datapoint is generated by a latent random variable $z_i$. There is also a parameter $\\theta$, which is global in the sense that all the datapoints depend on it.\n",
    "\n",
    "<img src=\"https://pyro.ai/examples/_static/img/vae_model.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "We allow $x_i$ to depend on $z_i$ in a complex, non-linear way, and this mapping is usually implemented with a deep neural network with parameters $\\theta$. This complexity makes the models more powerful, but also makes inference more challenging. The joint distribution can be factorized as:\n",
    "$$\n",
    "    p(\\mathbf{x}, \\mathbf{z}) = \\prod_{i=1}^N p_{\\theta} (\\mathbf{x}_i \\mid \\mathbf{z}_i) p (\\mathbf{z}_i)\n",
    "$$\n",
    "\n",
    "The job of inference is to recover a \"sensible\" posterior over the latent variables. Just as $x_i$ depends on $z_i$ in a complex way, so might we expect the posterior over $z_i$s to be complex, and inference challenging. Working under the dome of variational inference, we need to define a family of distributions as potential posteriors over the latents. \n",
    "\n",
    "We'll use ***amortized inference***. In amortized inference we introduce a single set of variational parameters $\\phi$ for **ALL** data points. The alternative would be to have variational parameters $\\lambda_i$ for each data point. This would, however, make it impossible to scale to large datasets, as then the parameter space would also explode in size. To parameterize the variational family we'll use a deep neural network.\n",
    "\n",
    "<img src=\"https://pyro.ai/examples/_static/img/vae_guide.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb314e7b",
   "metadata": {},
   "source": [
    "# The MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afcc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48daab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads and batches MNIST \n",
    "def setup_data_loaders(batch_size=128, use_cuda=False):\n",
    "    root = './data'\n",
    "    download = True\n",
    "    \n",
    "    # Download the MNIST train and test datasets, and transform the images and labels to tensors\n",
    "    trans = transforms.ToTensor()\n",
    "    train_set = MNIST(root=root, train=True, transform=trans,\n",
    "                      download=download)\n",
    "    test_set = MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "    # Build the dataloaders for the train and test sets\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": use_cuda}\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    return train_loader, test_loader, train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    mnist_train_loader, \n",
    "    mnist_test_loader, \n",
    "    mnist_train_set, \n",
    "    mnist_test_set\n",
    ") = setup_data_loaders(batch_size=BATCH_SIZE, use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each dataset item is a Tuple of (pixel tensor, label)\n",
    "print(mnist_train_set[11][0].shape)\n",
    "print(mnist_train_set[11][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2870e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_img(pixels, label):    \n",
    "    pixels = pixels.numpy()\n",
    "    pixels = pixels.reshape((28, 28))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(f\"Label is {label}\")\n",
    "    plt.imshow(pixels, cmap=\"gray\")\n",
    "    \n",
    "for _ in range(5):\n",
    "    rand_item = random.choice(mnist_test_set)\n",
    "    pixels_tensor, label = rand_item\n",
    "    \n",
    "    plot_mnist_img(pixels_tensor, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0eaeb5",
   "metadata": {},
   "source": [
    "# Define the Encoder and Decoder Modules\n",
    "\n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward()` method.\n",
    "\n",
    "The `forward()` method of the encoder expects as argument a tensor of dimensions *batch_dim* x 28 x 28, and infers the mean and log variance of the latent space Gaussians. Conversely, the decoder takes a tensor of dimensions *batch_dim* x *latent_space_size* and reconstructs the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Setup three linear transformations\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        \n",
    "        # Setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward computation on the image x\n",
    "        # First shape the mini-batch to have pixels in the rightmost dimension\n",
    "        x = x.reshape(-1, 784)\n",
    "        \n",
    "        # Then compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(x))\n",
    "        \n",
    "        # Return a mean vector and a the log variance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_logvar = self.fc22(hidden)\n",
    "        \n",
    "        return z_loc, z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Setup the two linear transformations used\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, 784)\n",
    "        \n",
    "        # Setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(z))\n",
    "        \n",
    "        # Return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc21(hidden))\n",
    "        loc_img = loc_img.view(-1, 28, 28)\n",
    "        \n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac899e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(z_dim=50, hidden_dim=100)\n",
    "print(encoder)\n",
    "\n",
    "decoder = Decoder(z_dim=50, hidden_dim=100)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17237bde",
   "metadata": {},
   "source": [
    "# The Variational Autoencoder in PyTorch\n",
    "\n",
    "To allow the passage of the gradient, we use the reparameterization trick for the Gaussian distribution:\n",
    "$$\n",
    "    \\mathbf{z} = \\mathbf{\\mu} + \\mathbf{\\sigma} \\mathbf{\\epsilon}\n",
    "$$\n",
    "where $\\mathbf{\\epsilon} \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "Our loss function is the ELBO:\n",
    "$$\n",
    "    \\textrm{ELBO} = \\mathbb{E}_{q_{\\lambda} (\\mathbf{z})} \\left[ \\log p_{\\theta} (\\mathbf{x}, \\mathbf{z}) - \\log q_{\\lambda} (\\mathbf{z}) \\right]\n",
    "$$\n",
    "$$\n",
    "    \\textrm{ELBO} = \\mathbb{E}_{q_{\\lambda} (\\mathbf{z})} \\left[ \\log p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z}) \\right] - D_{\\text{KL}} \\left[ q_{\\lambda}(\\mathbf{z} \\mid \\mathbf{x}) \\mid \\mid p_{\\theta}(\\mathbf{z}) \\right]\n",
    "$$\n",
    "We've chosen as our variational family a mean-field Gaussian posterior for our latent space. We can thus derive and use the closed form ELBO for Gaussian latents (see [here](https://arxiv.org/pdf/1907.08956.pdf) for a full derivation):\n",
    "$$\n",
    "    \\mathcal{L} = - \\sum_{j=1}^J \\frac{1}{2} \\left[ 1 + \\log \\sigma_j^2 - \\sigma_j^2 - \\mu_j^2 \\right] - \\frac{1}{L} \\sum_l \\mathbb{E}_{z \\sim q(z \\mid x_l)} \\left[ \\log p (x_l \\mid z_l) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEPyTorch(nn.Module):\n",
    "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(z_dim, hidden_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim)\n",
    "        \n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into GPU memory\n",
    "            self.cuda()\n",
    "            \n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "    def forward(self, input_data: torch.Tensor):\n",
    "        mu, logvar = self.encoder(input_data)\n",
    "        latent_sample = self.reparameterize(mu, logvar)\n",
    "        reconstructed_inputs = self.decoder(latent_sample)\n",
    "        \n",
    "        return input_data, reconstructed_inputs, mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        return mu + std * eps\n",
    "    \n",
    "    \n",
    "    def loss_function(\n",
    "        self, input_data, reconstructions, mu, logvar, beta\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons_loss = torch_functional.mse_loss(reconstructions, input_data)\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + beta * kld_loss\n",
    "        \n",
    "        return {\"total_loss\": loss, \"reconstruction_loss\": recons_loss.detach(), \"KLD\": -kld_loss.detach()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccbf9c9",
   "metadata": {},
   "source": [
    "Now that we have our data and our model we can proceed with training.\n",
    "\n",
    "\n",
    "Inside the training loop, optimization happens in three steps:\n",
    " - Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    " - Backpropagate the prediction loss with a call to `loss.backward()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    " - Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cc850",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_pytorch = VAEPyTorch(z_dim=50, hidden_dim=100, use_cuda=USE_CUDA)\n",
    "optimizer = torch.optim.Adam(vae_pytorch.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d4a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    for batch_num, (pixels, _) in enumerate(mnist_train_loader):\n",
    "        if USE_CUDA:\n",
    "            pixels = pixels.to(\"cuda\")\n",
    "            # labels = labels.to(\"cuda\")\n",
    "        \n",
    "        # Do NOT use directly forward()!\n",
    "        input_data, reconstructed_inputs, mu, logvar = vae_pytorch(pixels)\n",
    "        losses = vae_pytorch.loss_function(\n",
    "            pixels.squeeze(), reconstructed_inputs, mu, logvar, 0.001\n",
    "        )\n",
    "        loss = losses[\"total_loss\"]\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_num % 400 == 0:\n",
    "            print(f\"epoch_num: {epoch_num + 1}; batch_num: {batch_num}; loss: {loss.item():>7f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_item = random.choice(mnist_test_set)\n",
    "pixels_tensor, label = rand_item\n",
    "\n",
    "if USE_CUDA:\n",
    "    _, reconstructed_inputs, _, _ = vae_pytorch(pixels_tensor.cuda())\n",
    "    reconstructed_pixels = reconstructed_inputs.detach().cpu()\n",
    "else:\n",
    "    _, reconstructed_inputs, _, _ = vae_pytorch(pixels_tensor)\n",
    "    reconstructed_pixels = reconstructed_inputs.detach()\n",
    "\n",
    "plot_mnist_img(pixels_tensor, label)\n",
    "plot_mnist_img(reconstructed_pixels, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d9aef",
   "metadata": {},
   "source": [
    "# The Variational Autoencoder in Pyro\n",
    "\n",
    "Probabilistic models in Pyro are specified as Python functions `model(*args, **kwargs)` that generate observed data from latent variables using special primitive functions whose behavior can be changed by Pyro’s internals depending on the high-level computation being performed.\n",
    "\n",
    "Specifically, the different mathematical pieces of `model()` are encoded via the mapping:\n",
    " + latent random variables - `pyro.sample`\n",
    " + observed random variables - `pyro.sample` with the `obs` keyword argument\n",
    " + learnable parameters - `pyro.param`\n",
    " + plates - `pyro.plate` context managers\n",
    " \n",
    "The basic idea behind VI is that we introduce a parameterized distribution $q_{\\phi}(\\mathbf{z})$ called the variational distribution that will serve as an approximation to the posterior. We can think of $\\phi$ as parameterizing a space or family of probability distributions. Our goal will be to find the (not necessarily unique) probability distribution in that space that is the best possible approximation to the posterior distribution.\n",
    "\n",
    "Just like the model, the guide is encoded as a stochastic function `guide()` that contains `pyro.sample` and `pyro.param` statements, but can't contain observed data. Pyro furthermore enforces that `model()` and `guide()` have the same call signature, i.e. both callables should take the same arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3634f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEPyro(nn.Module):\n",
    "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create the encoder and decoder networks\n",
    "        self.encoder = Encoder(z_dim, hidden_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    # Define the model p(x|z)p(z)\n",
    "    def model(self, x):\n",
    "        # register PyTorch module `decoder` with Pyro\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # setup hyperparameters for prior p(z)\n",
    "            z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
    "            z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
    "            \n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            \n",
    "            # decode the latent code z\n",
    "            loc_img = self.decoder(z).view(-1, 784)\n",
    "            # score against actual images\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n",
    "        \n",
    "    # Define the guide (i.e. variational distribution) q(z|x)\n",
    "    def guide(self, x):\n",
    "        # register PyTorch module `encoder` with Pyro\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "            z_loc, logvar = self.encoder(x)\n",
    "            z_scale = torch.exp(logvar)\n",
    "            \n",
    "            # sample the latent code z\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "\n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, x):\n",
    "        # encode image x\n",
    "        z_loc, logvar = self.encoder(x)\n",
    "        z_scale = torch.exp(logvar)\n",
    "        \n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        \n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, _ in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    \n",
    "    return total_epoch_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    \n",
    "    for x, _ in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x)\n",
    "    \n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    \n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.distributions.enable_validation(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c555a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = mnist_train_loader, mnist_test_loader\n",
    "\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "vae = VAEPyro(use_cuda=USE_CUDA)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch + 1, total_epoch_loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92740e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_item = random.choice(mnist_test_set)\n",
    "pixels_tensor, label = rand_item\n",
    "\n",
    "if USE_CUDA:\n",
    "    reconstructed_inputs = vae.reconstruct_img(pixels_tensor.cuda())\n",
    "    reconstructed_pixels = reconstructed_inputs.detach().cpu()\n",
    "else:\n",
    "    reconstructed_inputs = vae.reconstruct_img(pixels_tensor)\n",
    "    reconstructed_pixels = reconstructed_inputs.detach()\n",
    "\n",
    "plot_mnist_img(pixels_tensor, label)\n",
    "plot_mnist_img(reconstructed_pixels, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ebbd23",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.\n",
    "\n",
    "[2] Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518), 859-877.\n",
    "\n",
    "[3] Ranganath, R., Gerrish, S., & Blei, D. (2014, April). Black box variational inference. In Artificial intelligence and statistics (pp. 814-822). PMLR.\n",
    "\n",
    "[4] Nunn, N., & Puga, D. (2012). Ruggedness: The blessing of bad geography in Africa. Review of Economics and Statistics, 94(1), 20-36.\n",
    "\n",
    "[5] Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.\n",
    "\n",
    "\n",
    "***Code and examples adapted from:***\n",
    "\n",
    "https://pyro.ai/examples/intro_long.html\n",
    "\n",
    "https://pyro.ai/examples/svi_part_i.html\n",
    "\n",
    "https://pyro.ai/examples/svi_part_iii.html\n",
    "\n",
    "https://pyro.ai/examples/vae.html\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/intro.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
